# ğŸ¤– Code Mentor Bot

**Code Mentor Bot** is a local-first AI assistant that explains code snippets â€” fully offline, with no API calls or cloud dependencies.

Powered by:
- ğŸ§  Local LLMs via [Ollama](https://ollama.com/)
- ğŸ–¥ï¸ Lightweight [Streamlit](https://streamlit.io/) UI
- ğŸ§° Supports models like `phi`, `codellama`, and `mistral`

---

## ğŸ§ª Demo

![demo-gif](demo.gif)  
<sub>Live explanation of recursive code using a local model</sub>

---

## ğŸš€ Features

- âœï¸ Paste any Python (or general-purpose) code
- ğŸ¤– Get instant explanations using offline models
- ğŸ›ï¸ Switch between multiple models (phi, mistral, codellama)
- âš ï¸ Smart static checks (e.g., recursion without base case)
- ğŸ§ª Tested & CI-friendly with mocked LLM calls

---

## ğŸ› ï¸ How to Run

> âš ï¸ Python 3.10+ recommended

### 1. Clone and set up environment

```bash
git clone https://github.com/your-username/code-mentor-bot.git
cd code-mentor-bot
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Clone and set up environment
Install Ollama: https://ollama.com/download
Then run:

```bash
ollama pull phi
ollama pull codellama
ollama pull mistral
```

### 3. Run the app
```bash
streamlit run app.py
```
ğŸ’¡ Example Input
```python
def factorial(n):
    return n * factorial(n - 1)
```

ğŸ§ª Run Tests
```bash
pytest -m "not slow"        # Fast tests only
pytest                      # All tests, including slow LLM tests
```

âœ¨ Why This Project?
Built to demonstrate:

ğŸ”’ Offline-friendly AI toolin <br />
ğŸ§  LangChain-free LLM integration <br />
ğŸ§‘â€ğŸ’» Thoughtful UX for code understanding <br />
ğŸ“ˆ Recruiter-ready polish and structure <br />
